{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:30px; color: white; background-color: #0071CD\">\n",
    "    <center>\n",
    "        <img height=\"75\" width=\"310\" src=\"img/logoub.jpeg\"></img>\n",
    "    <p>\n",
    "        <h1>Programación Paralela</h1>\n",
    "        <h2>Spark I</h2>\n",
    "        <h3>Manuel Ernesto Martínez Martín</h3>\n",
    "        <h3>Josep Manuel Lopez Camuñas</h3>\n",
    "    </p>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macos installation\n",
    "\n",
    "#### You also need Java and Python\n",
    "\n",
    "#### Spark installation\n",
    "- brew install apache-spark\n",
    "\n",
    "#### Add to your \"~/.bashrc\" (or zsh), X.Y.Z is your version (you can check by using \"brew info apache-spark\")\n",
    "- export SPARK_HOME=\"/usr/local/Cellar/apache-spark/X.Y.Z/libexec/\"\n",
    "- export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "- export PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "\n",
    "#### You can start jupyter notebook now by using \"pyspark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark, time\n",
    "from pyspark.sql.functions import expr, max, min, avg, col, sum, asc, desc\n",
    "import platform\n",
    "# In case of Windows\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        import findspark\n",
    "    except:\n",
    "        !pip install findspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    findspark.init()\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"Spark_Tutorial1/2007.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestra el Schema y el número de particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Particiones:\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"-\"*60)\n",
    "print(\"Particiones:\")\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar y seleccionar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|\n",
      "+------+----+--------+--------+\n",
      "|   SMF| ONT|       1|       7|\n",
      "|   SMF| PDX|       8|      13|\n",
      "|   SMF| PDX|      34|      36|\n",
      "|   SMF| PDX|      26|      30|\n",
      "|   SMF| PDX|      -3|       1|\n",
      "|   SMF| PDX|       3|      10|\n",
      "|   SMF| PHX|      47|      56|\n",
      "|   SMF| PHX|      -2|       9|\n",
      "|   SMF| PHX|      44|      47|\n",
      "|   SMF| PHX|      -7|       3|\n",
      "|   SMF| PHX|     -11|       1|\n",
      "|   SMF| PHX|      52|      52|\n",
      "|   SMF| SAN|      45|      53|\n",
      "|   SMF| SAN|     -17|      -5|\n",
      "|   SMF| SAN|      -5|       6|\n",
      "|   SMF| SAN|      33|      44|\n",
      "|   SMF| SAN|      -9|       0|\n",
      "|   SMF| SAN|      -7|       2|\n",
      "|   SMF| SAN|     -11|       1|\n",
      "|   SMF| SAN|      36|      29|\n",
      "+------+----+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop(\"FlightNum\",\"TailNum\",\"UniqueCarrier\")\n",
    "df2 = df.select(\"Origin\", \"Dest\", \"ArrDelay\", \"DepDelay\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminamos filas que tengan NA en ArrDelay o DepDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Añadir columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|DepDelay|ArrDelay|SumDelay|\n",
      "+--------+--------+--------+\n",
      "|       7|       1|       8|\n",
      "|      13|       8|      21|\n",
      "|      36|      34|      70|\n",
      "|      30|      26|      56|\n",
      "|       1|      -3|      -2|\n",
      "|      10|       3|      13|\n",
      "|      56|      47|     103|\n",
      "|       9|      -2|       7|\n",
      "|      47|      44|      91|\n",
      "|       3|      -7|      -4|\n",
      "+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.withColumn(\"SumDelay\", expr(\"ArrDelay + DepDelay\"))\n",
    "df4.select(\"DepDelay\", \"ArrDelay\", \"SumDelay\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max, Min y Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------------+\n",
      "|max(SumDelay)|min(SumDelay)|    avg(SumDelay)|\n",
      "+-------------+-------------+-----------------+\n",
      "|         5199|         -617|21.55425998256014|\n",
      "+-------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.select(max(\"SumDelay\"),min(\"SumDelay\"),avg(\"SumDelay\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento temporal en memoria caché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Origin: string, Dest: string, ArrDelay: int, DepDelay: int, SumDelay: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones de filtraje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|SumDelay|\n",
      "+------+----+--------+--------+--------+\n",
      "|   SMF| PDX|      -3|       1|      -2|\n",
      "|   SMF| PHX|      -7|       3|      -4|\n",
      "|   SMF| PHX|     -11|       1|     -10|\n",
      "|   SMF| SAN|     -17|      -5|     -22|\n",
      "|   SMF| SAN|      -9|       0|      -9|\n",
      "|   SMF| SAN|      -7|       2|      -5|\n",
      "|   SMF| SAN|     -11|       1|     -10|\n",
      "|   SMF| SAN|      -6|       3|      -3|\n",
      "|   SMF| SAN|     -14|       0|     -14|\n",
      "|   SMF| SAN|      -9|      -5|     -14|\n",
      "|   SMF| SNA|      -4|       0|      -4|\n",
      "|   SMF| SNA|      -8|       2|      -6|\n",
      "|   SMF| SNA|     -16|      -4|     -20|\n",
      "|   SMF| SNA|      -7|       0|      -7|\n",
      "|   SMF| SNA|     -15|      -4|     -19|\n",
      "|   SNA| MDW|     -18|       0|     -18|\n",
      "|   SNA| OAK|      -1|       0|      -1|\n",
      "|   SNA| OAK|       0|      -1|      -1|\n",
      "|   SNA| OAK|      -2|       0|      -2|\n",
      "|   SNA| OAK|      -1|       0|      -1|\n",
      "+------+----+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Unsupported class file major version 55'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\r\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\r\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\r\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\r\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\r\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\r\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\r\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\r\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-caf7d7afd623>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m print(\"df3 count: %d\\n\\\n\u001b[1;32m----> 5\u001b[1;33m df5 count: %d\"%(df3.count(), df5.count()))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m         \"\"\"\n\u001b[1;32m--> 523\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'Unsupported class file major version 55'"
     ]
    }
   ],
   "source": [
    "df5 = df4.where(\"SumDelay < 0\")\n",
    "#df5.show()\n",
    "\n",
    "print(\"df3 count: %d\\n\\\n",
    "df5 count: %d\"%(df3.count(), df5.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.where(\"SumDelay < 0\").where(\"Origin == 'JFK'\")\n",
    "#df5.show()\n",
    "\n",
    "print(\"df5 count: %d\"%(df5.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mismo filtraje de otra forma (se pueden usar variables de python aqui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "city = \"JFK\"\n",
    "df5 = df4.filter(col(\"SumDelay\") < i).filter(col(\"Origin\") == city)\n",
    "#df5.show()\n",
    "\n",
    "print(\"df5 count: %d\"%(df5.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.where(\"Origin == 'JFK'\")\n",
    "df5.select(sum(\"SumDelay\"),max(\"SumDelay\"),min(\"SumDelay\"),avg(\"SumDelay\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ascendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.sort(asc(\"SumDelay\"))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.sort(desc(\"SumDelay\")).limit(5)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener elementos unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.select(\"Origin\").distinct()\n",
    "print(df5.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Destinos diferentes: %d\"%df4.select(\"Dest\").distinct().count())\n",
    "print(\"Combinaciones de origen/destino unicas: %d\"%df4.select(\"Origin\",\"Dest\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceso a los datos desde python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = df4.limit(5).collect()\n",
    "print(datos)\n",
    "print(\"-\"*60)\n",
    "print(datos[0])\n",
    "print(\"-\"*60)\n",
    "print(datos[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura de ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Particiones %d = numero de ficheros\"%df4.rdd.getNumPartitions())\n",
    "df4_one = df4.coalesce(1) # para no crear n ficheros\n",
    "print(\"Particiones %d = numero de ficheros\"%df4_one.rdd.getNumPartitions())\n",
    "\n",
    "df4_one.write.format(\"csv\").option(\"header\", \"true\").save(\"Spark_Tutorial1/df4_one.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuantas más particiones se tengan más rápido ha de ir por que cada partición la usa un core en el caso de que se ejecute todo en un solo ordenador, pero en nuestro caso al usar un dual core el margen de mejora es insignificante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df4_sorted = df4.sort(asc(\"SumDelay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df4_one_sorted = df4_one.sort(asc(\"SumDelay\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
